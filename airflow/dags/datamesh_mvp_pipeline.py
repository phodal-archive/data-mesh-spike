"""
Data Mesh MVP Pipeline
======================
è¿™ä¸ª DAG æ¼”ç¤ºäº† Data Mesh æ¶æ„ä¸­çš„æ•°æ®ç®¡é“ï¼š
1. ä»å„ä¸ªåŸŸæå–æ•°æ®
2. åˆ›å»ºè·¨åŸŸæ•°æ®äº§å“
3. ç”Ÿæˆä¸šåŠ¡æŠ¥å‘Š
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
import mysql.connector
from airflow.exceptions import AirflowException
import os
import urllib.request
import urllib.error

# DAG é»˜è®¤å‚æ•°
default_args = {
    'owner': 'datamesh-team',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# åˆ›å»º DAG
dag = DAG(
    'datamesh_mvp_pipeline',
    default_args=default_args,
    description='Data Mesh MVP - è·¨åŸŸæ•°æ®äº§å“ç®¡é“',
    schedule_interval='@daily',
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['datamesh', 'mvp', 'data-product'],
)


def log_pipeline_start(**context):
    """è®°å½•ç®¡é“å¼€å§‹"""
    print("=" * 50)
    print("Data Mesh MVP Pipeline Started")
    print(f"Execution Date: {context['ds']}")
    print("=" * 50)
    return "Pipeline started successfully"


def validate_data_quality(**context):
    """
    éªŒè¯æ•°æ®è´¨é‡ - Data Mesh è´¨é‡å·¦ç§»å®è·µ
    åŒ…å«ä¸‰ç±»è§„åˆ™ï¼šå®Œæ•´æ€§ã€ä¸€è‡´æ€§ã€æ–°é²œåº¦
    """
    print("=" * 60)
    print("ğŸ” Data Quality Validation Started")
    print("=" * 60)
    
    # è¿æ¥ MariaDB
    conn = mysql.connector.connect(
        host='datamesh-mariadb',
        user='datamesh',
        password='datamesh123',
        database='domain_customers'
    )
    cursor = conn.cursor()
    
    failed_checks = []
    passed_checks = []
    
    # ===== å®Œæ•´æ€§æ£€æŸ¥ (Completeness) =====
    print("\nğŸ“‹ 1. Completeness Checks")
    print("-" * 60)
    
    # æ£€æŸ¥ 1: å®¢æˆ·ä¸»é”®éç©ºä¸”å”¯ä¸€
    cursor.execute("""
        SELECT 
            COUNT(*) as total,
            COUNT(DISTINCT customer_id) as unique_ids,
            COUNT(CASE WHEN customer_id IS NULL THEN 1 END) as null_ids
        FROM domain_customers.customers
    """)
    result = cursor.fetchone()
    total, unique_ids, null_ids = result
    
    check_name = "customers.customer_id: éç©ºä¸”å”¯ä¸€"
    if null_ids == 0 and total == unique_ids:
        passed_checks.append(check_name)
        print(f"  âœ“ {check_name}")
        print(f"    Total: {total}, Unique: {unique_ids}, Null: {null_ids}")
    else:
        failed_checks.append(check_name)
        print(f"  âœ— {check_name}")
        print(f"    Total: {total}, Unique: {unique_ids}, Null: {null_ids}")
    
    # æ£€æŸ¥ 2: è®¢å•å¿…é¡»æœ‰æœ‰æ•ˆå®¢æˆ·
    cursor.execute("""
        SELECT COUNT(*) as orphan_orders
        FROM domain_orders.orders o
        LEFT JOIN domain_customers.customers c ON o.customer_id = c.customer_id
        WHERE c.customer_id IS NULL
    """)
    orphan_orders = cursor.fetchone()[0]
    
    check_name = "orders: æ‰€æœ‰è®¢å•æœ‰æœ‰æ•ˆå®¢æˆ·"
    if orphan_orders == 0:
        passed_checks.append(check_name)
        print(f"  âœ“ {check_name}")
    else:
        failed_checks.append(check_name)
        print(f"  âœ— {check_name} - å‘ç° {orphan_orders} ä¸ªå­¤å„¿è®¢å•")
    
    # æ£€æŸ¥ 3: äº§å“ä»·æ ¼å¿…é¡»ä¸ºæ­£
    cursor.execute("""
        SELECT COUNT(*) as invalid_price_count
        FROM domain_products.products
        WHERE price IS NULL OR price <= 0
    """)
    invalid_prices = cursor.fetchone()[0]
    
    check_name = "products.price: å¿…é¡»ä¸ºæ­£æ•°"
    if invalid_prices == 0:
        passed_checks.append(check_name)
        print(f"  âœ“ {check_name}")
    else:
        failed_checks.append(check_name)
        print(f"  âœ— {check_name} - å‘ç° {invalid_prices} ä¸ªæ— æ•ˆä»·æ ¼")
    
    # ===== ä¸€è‡´æ€§æ£€æŸ¥ (Consistency) =====
    print("\nğŸ”— 2. Consistency Checks")
    print("-" * 60)
    
    # æ£€æŸ¥ 4: è®¢å•æ€»é¢ = è®¢å•æ˜ç»†æ±‡æ€»ï¼ˆå…³é”®ï¼ï¼‰
    cursor.execute("""
        SELECT 
            o.order_id,
            o.total_amount as order_total,
            COALESCE(SUM(oi.quantity * oi.unit_price), 0) as items_total,
            ABS(o.total_amount - COALESCE(SUM(oi.quantity * oi.unit_price), 0)) as diff
        FROM domain_orders.orders o
        LEFT JOIN domain_orders.order_items oi ON o.order_id = oi.order_id
        GROUP BY o.order_id, o.total_amount
        HAVING ABS(o.total_amount - COALESCE(SUM(oi.quantity * oi.unit_price), 0)) > 0.01
    """)
    inconsistent_orders = cursor.fetchall()
    
    check_name = "orders.total_amount = SUM(order_items)"
    if len(inconsistent_orders) == 0:
        passed_checks.append(check_name)
        print(f"  âœ“ {check_name}")
    else:
        failed_checks.append(check_name)
        print(f"  âœ— {check_name} - å‘ç° {len(inconsistent_orders)} ä¸ªä¸ä¸€è‡´è®¢å•:")
        for order_id, order_total, items_total, diff in inconsistent_orders[:3]:
            print(f"    Order {order_id}: è®¢å•={order_total}, æ˜ç»†={items_total}, å·®å¼‚={diff}")
        if len(inconsistent_orders) > 3:
            print(f"    ... è¿˜æœ‰ {len(inconsistent_orders) - 3} ä¸ª")
    
    # æ£€æŸ¥ 5: è®¢å•æ˜ç»†çš„äº§å“å¿…é¡»å­˜åœ¨
    cursor.execute("""
        SELECT COUNT(*) as invalid_products
        FROM domain_orders.order_items oi
        LEFT JOIN domain_products.products p ON oi.product_id = p.product_id
        WHERE p.product_id IS NULL
    """)
    invalid_products = cursor.fetchone()[0]
    
    check_name = "order_items: äº§å“å¿…é¡»å­˜åœ¨äº products"
    if invalid_products == 0:
        passed_checks.append(check_name)
        print(f"  âœ“ {check_name}")
    else:
        failed_checks.append(check_name)
        print(f"  âœ— {check_name} - å‘ç° {invalid_products} ä¸ªæ— æ•ˆäº§å“å¼•ç”¨")
    
    # ===== æ–°é²œåº¦æ£€æŸ¥ (Freshness) =====
    print("\nâ° 3. Freshness Checks")
    print("-" * 60)
    
    # æ£€æŸ¥ 6: æœ€è¿‘ 24 å°æ—¶å†…æœ‰è®¢å•æ›´æ–°
    cursor.execute("""
        SELECT 
            MAX(order_date) as last_order_date,
            TIMESTAMPDIFF(HOUR, MAX(order_date), NOW()) as hours_since_last_order
        FROM domain_orders.orders
    """)
    result = cursor.fetchone()
    last_order_date, hours_since = result[0], result[1] if result[1] is not None else 999
    
    check_name = "orders: 24å°æ—¶å†…æœ‰æ–°æ•°æ®"
    # å¯¹äºæ¼”ç¤ºï¼Œæˆ‘ä»¬æ”¾å®½åˆ° 72 å°æ—¶
    if hours_since <= 72:
        passed_checks.append(check_name)
        print(f"  âœ“ {check_name}")
        print(f"    æœ€åè®¢å•æ—¶é—´: {last_order_date} ({hours_since} å°æ—¶å‰)")
    else:
        failed_checks.append(check_name)
        print(f"  âš  {check_name}")
        print(f"    æœ€åè®¢å•æ—¶é—´: {last_order_date} ({hours_since} å°æ—¶å‰)")
    
    cursor.close()
    conn.close()
    
    # ===== æ±‡æ€»ç»“æœ =====
    print("\n" + "=" * 60)
    print("ğŸ“Š Quality Check Summary")
    print("=" * 60)
    print(f"âœ“ Passed: {len(passed_checks)}/{len(passed_checks) + len(failed_checks)}")
    print(f"âœ— Failed: {len(failed_checks)}/{len(passed_checks) + len(failed_checks)}")
    
    if failed_checks:
        print("\nâŒ Failed Checks:")
        for check in failed_checks:
            print(f"  - {check}")
    
    print("=" * 60)
    
    # æ¨é€ç»“æœåˆ° XComï¼ˆå¯è¢«ä¸‹æ¸¸ä»»åŠ¡ä½¿ç”¨ï¼‰
    quality_result = {
        'passed': passed_checks,
        'failed': failed_checks,
        'pass_rate': len(passed_checks) / (len(passed_checks) + len(failed_checks)) * 100
    }

    # Best-effort: push quality summary to Prometheus Pushgateway (for Grafana trends)
    total_checks = len(passed_checks) + len(failed_checks)
    _push_quality_metrics(
        {
            "datamesh_quality_pass_rate": quality_result["pass_rate"],
            "datamesh_quality_failed_checks": len(failed_checks),
            "datamesh_quality_total_checks": total_checks,
        }
    )
    
    # å¦‚æœæœ‰å¤±è´¥çš„å…³é”®æ£€æŸ¥ï¼ŒæŠ›å‡ºå¼‚å¸¸é˜»æ­¢ç®¡é“ç»§ç»­
    critical_failures = [f for f in failed_checks if 'è®¢å•æ€»é¢' in f or 'ä¸»é”®' in f]
    if critical_failures:
        raise AirflowException(
            f"âŒ å…³é”®è´¨é‡æ£€æŸ¥å¤±è´¥ï¼ç®¡é“å·²é˜»æ­¢ã€‚å¤±è´¥é¡¹: {', '.join(critical_failures)}"
        )
    
    return quality_result


def generate_kpi_report(**context):
    """ç”Ÿæˆ KPI æŠ¥å‘Š"""
    print("Generating Business KPI Report...")
    print("-" * 40)
    
    # æ¨¡æ‹Ÿ KPI æ•°æ®ï¼ˆå®é™…ä¸­ä¼šä»æ•°æ®åº“æŸ¥è¯¢ï¼‰
    kpis = {
        'total_customers': 10,
        'total_orders': 13,
        'total_revenue': 3500.00,
        'avg_order_value': 269.23,
        'pending_orders': 3,
    }
    
    for kpi, value in kpis.items():
        print(f"  {kpi}: {value}")
    
    print("-" * 40)
    print("KPI Report generated successfully!")
    return kpis


def _read_sql_statements(sql_path: str) -> list[str]:
    """Very small SQL loader for .sql files with '--' comments and ';' terminators."""
    with open(sql_path, "r", encoding="utf-8") as f:
        lines = f.readlines()

    statements: list[str] = []
    buf: list[str] = []

    for raw in lines:
        line = raw.strip()
        if not line:
            continue
        if line.startswith("--"):
            continue
        # Keep original raw (preserve spacing) for view definitions readability
        buf.append(raw)
        if ";" in raw:
            joined = "".join(buf).strip()
            # Split in case there are multiple statements on one line
            parts = joined.split(";")
            for part in parts[:-1]:
                stmt = part.strip()
                if stmt:
                    statements.append(stmt)
            # Remaining part (if any) continues buffer
            remainder = parts[-1].strip()
            buf = [remainder + "\n"] if remainder else []

    tail = "".join(buf).strip()
    if tail:
        statements.append(tail)

    return statements


def refresh_data_products_views(**context):
    """
    Create/refresh Data Products (dp_*) views in MariaDB.
    This makes the MVP deterministic: dp_* exists without manual SQL steps.
    """
    print("=" * 60)
    print("ğŸ§© Refreshing Data Products (dp_*)")
    print("=" * 60)

    dag_dir = os.path.dirname(os.path.abspath(__file__))
    sql_path = os.path.join(dag_dir, "sql", "03-data-products.sql")
    if not os.path.exists(sql_path):
        raise AirflowException(f"Data Products SQL not found: {sql_path}")

    statements = _read_sql_statements(sql_path)
    if not statements:
        raise AirflowException(f"No SQL statements parsed from: {sql_path}")

    conn = mysql.connector.connect(
        host='datamesh-mariadb',
        user='datamesh',
        password='datamesh123',
        database='domain_analytics',
        autocommit=True,
    )
    cursor = conn.cursor()

    current_db = "domain_analytics"
    applied = 0
    try:
        for stmt in statements:
            upper = stmt.strip().upper()
            if upper.startswith("USE "):
                db = stmt.split(None, 1)[1].strip()
                cursor.execute(f"USE {db}")
                current_db = db
                print(f"â†’ Using database: {current_db}")
                continue

            cursor.execute(stmt)
            applied += 1
        print(f"âœ… Applied {applied} SQL statements from {sql_path}")
    finally:
        cursor.close()
        conn.close()

    return {"applied_statements": applied, "sql_path": sql_path}


def _push_quality_metrics(metrics: dict) -> None:
    """Push quality metrics to Prometheus Pushgateway (best-effort)."""
    endpoint = os.environ.get("PUSHGATEWAY_URL", "http://pushgateway:9091")
    job = "datamesh_mvp_pipeline"
    url = f"{endpoint}/metrics/job/{job}"

    lines = []
    for k, v in metrics.items():
        # Prometheus metric name safety
        name = k.strip()
        lines.append(f"{name} {v}")
    body = ("\n".join(lines) + "\n").encode("utf-8")

    req = urllib.request.Request(url, data=body, method="PUT")
    req.add_header("Content-Type", "text/plain; version=0.0.4; charset=utf-8")
    try:
        with urllib.request.urlopen(req, timeout=5) as resp:
            print(f"ğŸ“¤ Pushed quality metrics to Pushgateway: HTTP {resp.status}")
    except (urllib.error.URLError, urllib.error.HTTPError) as e:
        # Don't fail the pipeline just because monitoring is down in MVP
        print(f"âš ï¸  Failed to push metrics to Pushgateway ({url}): {e}")


def notify_data_products_ready(**context):
    """é€šçŸ¥æ•°æ®äº§å“å·²å°±ç»ª"""
    print("=" * 50)
    print("Data Products Ready for Consumption:")
    print("  - dp_customer_360: Customer 360 View")
    print("  - dp_product_sales: Product Sales Analytics")
    print("  - dp_order_fulfillment: Order Fulfillment Status")
    print("  - dp_business_kpis: Business KPI Dashboard")
    print("=" * 50)
    return "Notification sent"


# å®šä¹‰ä»»åŠ¡
start_task = PythonOperator(
    task_id='start_pipeline',
    python_callable=log_pipeline_start,
    dag=dag,
)

validate_quality = PythonOperator(
    task_id='validate_data_quality',
    python_callable=validate_data_quality,
    dag=dag,
)

# åˆ›å»º/åˆ·æ–°æ•°æ®äº§å“ï¼ˆdp_*ï¼‰
refresh_data_products_task = PythonOperator(
    task_id='refresh_data_products',
    python_callable=refresh_data_products_views,
    dag=dag,
)

generate_report = PythonOperator(
    task_id='generate_kpi_report',
    python_callable=generate_kpi_report,
    dag=dag,
)

notify_ready = PythonOperator(
    task_id='notify_data_products_ready',
    python_callable=notify_data_products_ready,
    dag=dag,
)

# å®šä¹‰ä»»åŠ¡ä¾èµ–å…³ç³»
start_task >> validate_quality >> refresh_data_products_task >> generate_report >> notify_ready

